# Paper Reading List

index | Title | Source | Notes/Code |
------------ | ------------- | ------------ | ------------- |
1 | *Structured and Unstructured Cache Models for SMT Domain Adaptation* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Structured-and-Unstructured-Cache-Models-for-SMT-Domain-Adaptation.pdf) | [Corpus](http://homepages.inf.ed.ac.uk/alouis/wikiBio.html) |
2 | *Exploiting Cross-Sentence Context for Neural Machine Translation* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Exploiting-Cross-Sentence-Context-for-Neural-Machine-Translation.pdf) | [Code1/](https://github.com/tuzhaopeng/LC-NMT)[Code2](https://github.com/longyuewangdcu/Cross-Sentence-NMT) |
3 | *Attention Is All You Need* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Attention-Is-All-You-Need.pdf) | [Note/](http://nlp.seas.harvard.edu/2018/04/03/attention.html)[Code](https://github.com/tensorflow/tensor2tensor) |
4 | *NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/NEURAL-MACHINE-TRANSLATION-BY-JOINTLY-LEARNING-TO-ALIGN-AND-TRANSLATE.pdf) | [Code](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb) |
5 | *Sequence to Sequence Learning with Neural Networks* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Sequence-to-Sequence-Learning-with-Neural-Networks.pdf) | [Code](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) |
6 | *Effective Approaches to Attention-based Neural Machine Translation* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Effective-Approaches-to-Attention-based-Neural-Machine-Translation.pdf) | [Code1/](https://github.com/lmthang/nmt.hybrid)[Code2](https://github.com/dillonalaird/Attention) |
7 | *Asynchronous Bidirectional Decoding for Neural Machine Translation* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Asynchronous-Bidirectional-Decoding-for-Neural-Machine-Translation.pdf) | [Code](https://github.com/DeepLearnXMU/ABD-NMT) |
8 | *Convolutional Sequence to Sequence Learning* | [source](https://github.com/Eurus-Holmes/Research_Papers/blob/master/papers/Convolutional-Sequence-to-Sequence-Learning.pdf) | [Code1/](https://github.com/facebookresearch/fairseq)[Code2](https://github.com/pytorch/fairseq) |



----
# Paper Reading Todo List

* Jörg Tiedemann, and Yves Scherrer. 2017. [Neural Machine Translation with Extended Context](http://www.aclweb.org/anthology/W17-4811). In *Proceedings of the Third Workshop on Discourse in Machine Translation*. ([Citation](https://scholar.google.com/scholar?um=1&ie=UTF-8&lr&cites=16950693252825831302): 12)
* Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. [Evaluating Discourse Phenomena in Neural Machine Translation](http://aclweb.org/anthology/N18-1118). In *Proceedings of NAACL 2018*. ([Citation](https://scholar.google.com/scholar?cites=1436848483757205177&as_sdt=2005&sciodt=0,5&hl=en): 11)
* Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. [Context-Aware Neural Machine Translation Learns Anaphora Resolution](http://aclweb.org/anthology/P18-1117). In *Proceedings of ACL 2018*. ([Citation](https://scholar.google.com/scholar?cites=16594777811418303416&as_sdt=2005&sciodt=0,5&hl=en): 7)
* Sameen Maruf and Gholamreza Haffari. 2018. [Document Context Neural Machine Translation with Memory Networks](http://aclweb.org/anthology/P18-1118). In *Proceedings of ACL 2018*. ([Citation](https://scholar.google.com/scholar?cites=17337605639464710308&as_sdt=2005&sciodt=0,5&hl=en): 5)
* Shaohui Kuang, Deyi Xiong, Weihua Luo, Guodong Zhou. 2018. [Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches](http://aclweb.org/anthology/C18-1050). In *Proceedings of COLING 2018*. ([Citation](https://scholar.google.com/scholar?cites=12991114209233735355&as_sdt=2005&sciodt=0,5&hl=en): 1)
* Shaohui Kuang and Deyi Xiong. 2018. [Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model](https://arxiv.org/pdf/1806.04466.pdf). In *Proceedings of COLING 2018*.
* Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang and Yang Liu. 2018. [Improving the Transformer Translation Model with Document-Level Context](http://aclweb.org/anthology/D18-1049). In *Proceedings of EMNLP 2018*.
* Samuel Läubli, Rico Sennrich, and Martin Volk. 2018. [Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation](http://aclweb.org/anthology/D18-1512). In *Proceedings of EMNLP 2018*. ([Citation](https://scholar.google.com/scholar?cites=13135618112238453725&as_sdt=2005&sciodt=0,5&hl=en): 1)
* Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. [Document-Level Neural Machine Translation with Hierarchical Attention Networks](http://aclweb.org/anthology/D18-1325). In *Proceedings of EMNLP 2018*.
* Zhaopeng Tu, Yang Liu, Shumin Shi, and Tong Zhang. 2018. [Learning to Remember Translation History with a Continuous Cache](https://arxiv.org/pdf/1711.09367.pdf). *Transactions of the Association for Computational Linguistics*. ([Citation](https://scholar.google.com/scholar?cites=15854294745619374487&as_sdt=2005&sciodt=0,5&hl=en): 9)
